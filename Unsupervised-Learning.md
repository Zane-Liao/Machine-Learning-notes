- In contrast to supervised learning, unsupervised learning does not rely on labeled data for training.
- The most common unsupervised learning algorithms include PCA, ICA, k-means, EM, and self-supervised learning.

[[#Clustering]]
[[#k-means algorithm]]
[[#Principal Component Analysis (PCA)]]
[[#Independent Component Analysis (ICA)]]
[[#EM algorithms]]
[[#Semi-Supervised Learning (EM + GMM)]]
[[#Gaussian Mixture Models (GMM)]]
[[#Self-supervised Learning]]

##### Define
- Given a set of samples $\{x_1, x_2, \dots, x_n\}$, which are independent and identically distributed from an unknown distribution $P_X$, the goal of unsupervised learning is to learn the underlying structure or generative distribution of the data without supervisory signals (labels, rewards), thereby obtaining an approximate model $\hat{P}_X$. , or a function $f(x)$ that learns the underlying patterns in the data.

##### Clustering
- What is clustering? Clustering is the process of dividing a collection of unlabeled data into different groups (clusters). Data within the same group is highly similar, while data from different groups differ significantly.
- Common clustering algorithms include the k-means algorithm and the Gaussian Mixture Model (GMM).

##### k-means algorithm
- Steps of the k-means algorithm
![[Screenshot 2025-09-21 17.31.37.png]]

##### Principal Component Analysis (PCA)
- Principal component analysis, finds one group of new orthogonal coordinate axes so that the variance of the data projections on these axes is large.
- Dimensionality reduction + decorrelation
- We use eigenvalue decomposition or SVD of the covariance matrix to obtain principal components.
##### Independent Component Analysis (ICA)
- Decomposes the observed signal into a set of independent statistical components.
- In blind source separation (BSS), we require not only decorrelation but also high-order independent statistics (such as non-Gaussianity).
- A well-known example is the cocktail party problem, where multiple microphones are separated into individual voices, and in financial quantitative trading, independent market factors are separated.
##### EM algorithms
- Expectation-Maximization Algorithm. In machine learning probabilistic models, there are many latent variables. In mixture models, it is unknown which cluster a sample belongs to. Maximizing the likelihood function (MLE) is often difficult because latent variables are unobservable.
- EM Algorithm Steps
1. Estimate the distribution of the latent variable using the current parameters (E-step)
2. Under this distribution, maximize the expected log-likelihood and update the parameters (M-step)
3. Continue iterating until convergence
- EM algorithms are commonly used in GMM and HMM (Hidden Markov Model), as well as for handling missing data.

##### Semi-Supervised Learning (EM + GMM)
- We call semi-supervised learning (EM + GMM) Semi-supervised learning is a learning method in which some data sets are labeled, while others are not.
- We will only analyze GMM under semi-supervised learning.
- GMM assumes that data are generated by a mixture of K Gaussian distributions.
$$p(x|\theta) = \sum_{k=1}^K \pi_k \, \mathcal{N}(x|\mu_k, \Sigma_k)$$
- The GMM algorithm under EM requires estimating the parameters $\theta = \{\pi_k, \mu_k, \Sigma_k\}$
- The E-step (calculating the posterior probability that sample $x_i$ belongs to cluster k)
$$\gamma_{ik} = P(z_i = k | x_i, \theta^{(t)}) = \frac{\pi_k \, \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \, \mathcal{N}(x_i|\mu_j, \Sigma_j)}$$
- M-step (parameter update)
$$N_k = \sum_{i=1}^N \gamma_{ik}$$
$$\pi_k^{(t+1)} = \frac{N_k}{N}, \quad
\mu_k^{(t+1)} = \frac{1}{N_k} \sum_{i=1}^N \gamma_{ik} x_i$$
$$\Sigma_k^{(t+1)} = \frac{1}{N_k} \sum_{i=1}^N \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T$$
##### Self-supervised Learning
- For self-supervised learning, see [[Introduction to Deep Learning]]